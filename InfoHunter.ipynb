{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ccb618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install unstructured\n",
    "#brew install unoconv\n",
    "#pip install odfpy\n",
    "#pip install langchain\n",
    "#pip install chromadb\n",
    "#pip install pypdf\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "from PyPDF2 import PdfReader\n",
    "import subprocess  \n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "def read_file(file_path):\n",
    "    '''\n",
    "    This function reads a file among eligible file types:\n",
    "         {'.txt', '.pdf', '.odt', '.ods', '.docx', '.xlsx', '.ipynb', '.py', '.pptx', '.html'},\n",
    "    and converts the file into langchain.docstore.document.Document object\n",
    "    return: [Document(s)]\n",
    "    '''       \n",
    "\n",
    "    content, content_sheets, data, pages, doc = None, None, None, None, None\n",
    "\n",
    "    file_extension = os.path.splitext(file_path)[1]\n",
    "    \n",
    "    if file_extension in {'.txt', '.pdf', '.odt', '.ods', '.docx', '.xlsx', '.ipynb', '.py', '.pptx', '.html'}:\n",
    "\n",
    "        # Read the file based on its extension\n",
    "        if file_extension == '.pdf':\n",
    "            print(\"this is a pdf\")\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            pages = loader.load_and_split() # a list of Document objects, separate by page\n",
    "\n",
    "        elif file_extension == '.docx':\n",
    "            print(\"this is a docx\")\n",
    "            loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            data = loader.load() # a list of one Document object\n",
    "\n",
    "        elif file_extension == '.pptx':\n",
    "            print(\"this is a pptx\")\n",
    "            loader = UnstructuredPowerPointLoader(file_path)\n",
    "            data = loader.load() # a list of one Document object\n",
    "\n",
    "        elif file_extension == '.html':\n",
    "            print(\"this is a html\")\n",
    "            loader = UnstructuredHTMLLoader(file_path)\n",
    "            data = loader.load() # a list of one Document object\n",
    "\n",
    "        elif file_extension in ('.txt', '.py'):\n",
    "            print(\"this is a txt or py\")\n",
    "            loader = TextLoader(file_path, encoding='utf8')\n",
    "            data = loader.load() # a list of one Document object\n",
    "\n",
    "        elif file_extension == '.odt':\n",
    "            print(\"this is a odt\")\n",
    "            cmd = ['unoconv', '--stdout', '-f', 'txt', file_path]\n",
    "            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout, stderr = p.communicate()\n",
    "            content = stdout.decode('utf-8') # a string\n",
    "\n",
    "        elif file_extension == '.ipynb':\n",
    "            print(\"this is a ipynb\")\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                notebook = nbformat.read(f, as_version=nbformat.NO_CONVERT)\n",
    "            exporter = PythonExporter()\n",
    "            content, _ = exporter.from_notebook_node(notebook) # content is a string            \n",
    "\n",
    "        elif file_extension in ('.ods', '.xlsx'):\n",
    "            print(\"this is a ods or xlsx\")\n",
    "            excel_file = pd.ExcelFile(file_path)\n",
    "            # Iterate over sheets and extract data\n",
    "            content_sheets = {}\n",
    "            for sheet_name in excel_file.sheet_names:\n",
    "                content_sheets[sheet_name] = excel_file.parse(sheet_name) \n",
    "                # each sheet is saved as a DataFrame\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # save the file as a Document object\n",
    "        if data:\n",
    "            print(file_path, \"has data.\")\n",
    "            return data\n",
    "\n",
    "        elif pages:\n",
    "            print(file_path, \"has pages.\")\n",
    "            return pages\n",
    "\n",
    "        elif content:\n",
    "            print(file_path, \"has content.\")\n",
    "            content = content.replace(\"  \",\"\")\n",
    "            doc = Document(page_content=content, metadata={\"source\": file_path})\n",
    "            return [doc]\n",
    "\n",
    "        elif content_sheets:\n",
    "            print(file_path, \"has content_sheets.\")\n",
    "            docs = []\n",
    "            for sheet_name in content_sheets:\n",
    "                content_in_sheet = str(content_sheets[sheet_name])\n",
    "                # remove large amounts of space and NaN in xlsx and ods\n",
    "                content_in_sheet = content_in_sheet.replace(\"  \",\"\")\n",
    "                content_in_sheet = content_in_sheet.replace(\"NaN\", \"\")\n",
    "                sheet = Document(page_content=content_in_sheet, metadata={\"source\": file_path, \"sheet\": sheet_name})\n",
    "                docs.append(sheet)\n",
    "            return docs\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4168781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_recursion(folder_path):\n",
    "    \"\"\"\n",
    "    This function loops through the contents in a root folder:\n",
    "    if a content is a single file, run read_file();\n",
    "    if a content is a foler, recurse folder_recursion().\n",
    "    return: [Documents], all eligible document types in the root folder and all subfolders\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "    \n",
    "    # Loop through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if os.path.isdir(file_path): # file_path is a folder\n",
    "            documents += folder_recursion(file_path)\n",
    "        \n",
    "        else: # file_path is a single file\n",
    "            documents += read_file(file_path)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abde76f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jian/Documents/GitHub/PersonalNoteBot'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22502e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a odt\n",
      "../../../NLP_Kai/notes2/6_machine learning framework.odt has content.\n",
      "this is a docx\n",
      "../../../NLP_Kai/notes2/7_NLP models and tasks.docx has data.\n",
      "this is a ods or xlsx\n",
      "../../../NLP_Kai/notes2/4_important notes.xlsx has content_sheets.\n",
      "this is a docx\n",
      "../../../NLP_Kai/notes2/11_文件存在哪里save file.docx has data.\n",
      "this is a pdf\n",
      "../../../NLP_Kai/notes2/9_T5.pdf has pages.\n",
      "this is a txt or py\n",
      "../../../NLP_Kai/notes2/3_hw1.py has data.\n",
      "this is a txt or py\n",
      "../../../NLP_Kai/notes2/8_requirements.txt has data.\n",
      "this is a html\n",
      "../../../NLP_Kai/notes2/LangChain 0.0.147.html has data.\n",
      "this is a ipynb\n",
      "../../../NLP_Kai/notes2/1_Deep_Learning_Solution.ipynb has content.\n",
      "this is a odt\n",
      "../../../NLP_Kai/notes2/folder1/folder2/machine learning framework.odt has content.\n",
      "this is a odt\n",
      "../../../NLP_Kai/notes2/folder1/folder3/machine learning framework.odt has content.\n",
      "this is a ods or xlsx\n",
      "../../../NLP_Kai/notes2/folder1/folder3/目录.ods has content_sheets.\n",
      "this is a ods or xlsx\n",
      "../../../NLP_Kai/notes2/folder1/目录.ods has content_sheets.\n",
      "this is a pptx\n",
      "../../../NLP_Kai/notes2/2_Dickinson_Sample_Slides.pptx has data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = '../../../NLP_Kai/notes2/' #the folder directory containing your notes\n",
    "documents = folder_recursion(folder_path)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680e5c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 125 documents (and pages) from directory ../../../NLP_Kai/notes2/\n"
     ]
    }
   ],
   "source": [
    "print(f\"Read in {len(documents)} documents (and pages) from directory {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244f09d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split each document into a good size\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0be55bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7d6dfbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db2\n",
      "Using embedded DuckDB with persistence: data will be stored in: db2\n"
     ]
    }
   ],
   "source": [
    "# attention: we need persist_directory to save our embeddings for later use\n",
    "# OpenAI charges on embedding and run(query)\n",
    "\n",
    "# a folder named 'db2' is created inside folder_path, db2 has your embeddings\n",
    "\n",
    "persist_directory = 'db2'\n",
    "vectordb = Chroma.from_documents(documents=split_documents, embedding=embeddings, persist_directory=persist_directory)\n",
    "vectordb.persist() # need .persist() in ipynb to save to local drive. in python script, this is automatic\n",
    "vectordb = None\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings) # successfully load embedding from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "72c9e502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Pre-training a transformer typically involves using a large amount of unlabeled data, such as the Common Crawl Corpus (C4), and training for a long time using an \"inverse square root\" learning rate schedule. Additionally, increasing the batch size and the number of training steps can confer benefits.'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chatbot QA function\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type='stuff', retriever=vectordb.as_retriever())\n",
    "query = 'how to pre-train a transformer?'\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0d411b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search to return related documents\n",
    "similar_docs = vectordb.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "05733e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Longer training Our baseline model uses a relatively small amount of pre-training (1⁄4as\\nmuch as BERT (Devlin et al., 2018),1⁄16as much as XLNet (Yang et al., 2019),1⁄64as\\nmuch as RoBERTa (Liu et al., 2019c), etc.). Fortunately, C4 is big enough that we\\ncan train for substantially longer without repeating data (which can be detrimental,\\nas shown in Section 3.4.2). We found in Section 3.6 that additional pre-training can\\nindeed be helpful, and that both increasing the batch size and increasing the number of\\ntraining steps can confer this beneﬁt. We therefore pre-train our models for 1million\\nsteps on a batch size of 211sequences of length 512, corresponding to a total of about\\n1trillion pre-training tokens (about 32×as many as our baseline). In Section 3.4.1, we\\nshowed that pre-training on the RealNews-like, WebText-like, and Wikipedia + TBC\\ndata sets outperformed pre-training on C4 on a few downstream tasks. However, these', metadata={'source': 'notes/T5.pdf', 'page': 35}),\n",
       "  0.3898905813694),\n",
       " (Document(page_content='Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\\neﬀect of pre-training for more steps in Sections 3.6 and 3.7. Note that 235tokens only covers\\na fraction of the entire C4 data set, so we never repeat any data during pre-training.\\nDuringpre-training, weusean“inversesquareroot”learningrateschedule: 1/slashbig/radicalbig\\nmax(n,k)\\nwherenis the current training iteration and kis the number of warm-up steps (set to 104\\nin all of our experiments). This sets a constant learning rate of 0.01for the ﬁrst 104steps,\\nthen exponentially decays the learning rate until pre-training is over. We also experimented\\nwith using a triangular learning rate (Howard and Ruder, 2018), which produced slightly\\nbetter results but requires knowing the total number of training steps ahead of time. Since\\nwe will be varying the number of training steps in some of our experiments, we opt for the\\nmore generic inverse square root schedule.', metadata={'source': 'notes/T5.pdf', 'page': 11}),\n",
       "  0.3963688611984253),\n",
       " (Document(page_content='as many steps. Unsurprisingly, we ﬁnd that pre-training provides signiﬁcant gains across\\nalmost all benchmarks. The only exception is WMT English to French, which is a large\\nenough data set that gains from pre-training tend to be marginal. We include this task in\\nour experiments to test the behavior of transfer learning in the high-resource regime. Since\\nwe perform early stopping by selecting the best-performing checkpoint, the large disparity\\nbetween our baseline and “no pre-training” emphasize how much pre-training improves\\nperformance on tasks with limited data. While we do not explicitly measure improvements\\nin data eﬃciency in this paper, we emphasize that this is one of the primary beneﬁts of the\\ntransfer learning paradigm.\\n14', metadata={'source': 'notes/T5.pdf', 'page': 13}),\n",
       "  0.40343448519706726),\n",
       " (Document(page_content='following section.\\n3.4.2 Pre-training Data set Size\\nThe pipeline we use to create C4 was designed to be able to create extremely large pre-\\ntraining data sets. The access to so much data allows us to pre-train our models without\\nrepeating examples. It is not clear whether repeating examples during pre-training would\\nbe helpful or harmful to downstream performance because our pre-training objective is itself\\nstochastic and can help prevent the model from seeing the same exact data multiple times.\\nTo test the eﬀect of limited unlabeled data set sizes, we pre-trained our baseline model\\non artiﬁcially truncated versions of C4. Recall that we pre-train our baseline model on\\n235≈34Btokens (a small fraction of the total size of C4). We consider training on truncated\\nvariants of C4 consisting of 229,227,225and223tokens. These sizes correspond to repeating\\nthe data set 64,256,1,024, and 4,096times respectively over the course of pre-training.\\n27', metadata={'source': 'notes/T5.pdf', 'page': 26}),\n",
       "  0.4210672080516815)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will return metadata\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "36873f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No, we do not have a requirements.txt file.'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"do we have a requirements.txt file?\"\n",
    "qa.run(query)\n",
    "# the document name in metadata is NOT used in similarity search.\n",
    "# will add document name into the beginning of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2f0b26ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='tensorflow-io-gcs-filesystem==0.23.1\\ntensorflow-probability==0.16.0\\ntermcolor==1.1.0\\nterminado==0.16.0\\ntf-estimator-nightly==2.8.0.dev2021122109\\nthreadpoolctl==3.0.0\\ntinycss2==1.1.1\\ntokenizers==0.13.2\\ntoml==0.10.2\\ntoolz==0.12.0\\ntorch==1.12.0\\ntorchvision==0.13.0\\ntornado==6.2\\ntqdm==4.62.3\\ntraitlets==5.4.0\\ntransformers==4.27.4\\ntrio==0.19.0\\ntrio-websocket==0.9.2\\ntyping_extensions==4.0.1\\ntzdata==2022.2\\ntzlocal==4.2\\nurllib3==1.26.7\\nvalidators==0.20.0\\nvirtualenv==20.10.0\\nwcwidth==0.2.5\\nwebencodings==0.5.1\\nWerkzeug==2.0.2\\nwidgetsnbextension==4.0.3\\nwrapt==1.13.3\\nwsproto==1.0.0\\nxgboost==1.5.0\\nzipp==3.7.0', metadata={'source': 'notes/requirements.txt'}),\n",
       "  0.4120103716850281),\n",
       " (Document(page_content='ipykernel==6.16.0\\nipython==8.5.0\\nipython-genutils==0.2.0\\nipywidgets==8.0.2\\nitsdangerous==2.0.1\\njedi==0.18.1\\nJinja2==3.0.3\\njoblib==1.1.0\\njsonschema==4.16.0\\njupyter==1.0.0\\njupyter-console==6.4.4\\njupyter-core==4.11.1\\njupyter_client==7.3.5\\njupyterlab-pygments==0.2.2\\njupyterlab-widgets==3.0.3\\njupyterplot==0.0.3\\nkeras==2.8.0\\nKeras-Preprocessing==1.1.2\\nkiwisolver==1.3.2\\nlibclang==12.0.0\\nllvmlite==0.37.0\\nlrcurve==1.1.0\\nMarkdown==3.3.6\\nMarkupSafe==2.0.1\\nmatplotlib==3.5.1\\nmatplotlib-inline==0.1.6\\nmistune==2.0.4\\nnbclient==0.7.0\\nnbconvert==7.2.1\\nnbformat==5.6.1\\nnest-asyncio==1.5.6\\nnotebook==6.4.12\\nnumba==0.54.1\\nnumpy==1.19.5\\nnvidia-ml-py3==7.352.0\\noauthlib==3.1.1\\nopt-einsum==3.3.0\\noutcome==1.1.0\\npackaging==21.0\\npandas==1.2.4\\npandocfilters==1.5.0\\nparso==0.8.3\\npexpect==4.8.0\\npickleshare==0.7.5\\nPillow==8.4.0\\nplatformdirs==2.4.1\\nplotly==5.10.0\\npluggy==0.13.1\\nprometheus-client==0.14.1\\nprompt-toolkit==3.0.31\\nprotobuf==3.19.1\\npsutil==5.9.2\\nptyprocess==0.7.0\\npure-eval==0.2.2\\npy==1.10.0\\npyarrow==9.0.0', metadata={'source': 'notes/requirements.txt'}),\n",
       "  0.43379339575767517),\n",
       " (Document(page_content='# # Application', metadata={'source': 'notes/3_NLP_In_Depth_Solution.ipynb'}),\n",
       "  0.4625842571258545),\n",
       " (Document(page_content='#!/usr/bin/env python\\n# coding: utf-8\\n\\n# # NLP in depth', metadata={'source': 'notes/3_NLP_In_Depth_Solution.ipynb'}),\n",
       "  0.49318262934684753)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search_with_score(\"Flask==2.0.2\")\n",
    "# 只搜“Flask”找不到正确的doc，这里也没有找到requirements.txt里含有Flask的那一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bac29940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 文件的存储位置总是从working directory出发，而不是test.py的位置出发，使用Slash和back slash的方向在IBM和Mac中不一样，如果用r string则两个方向都可以，就像第二个存储路径的写法。'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"文件存在哪里\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a69ab269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='文件存在哪里？\\n\\n有如下路径：\\n\\nA\\\\B\\\\C\\\\D\\\\test.py \\tfolder D中有python file test.py\\n\\nA\\\\B\\\\data\\t\\tfolder B中有folder data\\n\\n我们的working directory是A\\\\B\\n\\n文件test.py中有存储命令：df.to_csv(“mytest.csv”)，那么这个df会被存到B folder下。\\n\\n如果把存储命令改为：df.to_csv(r“data\\\\mytest.csv”)，那么这个df会被存到B\\\\data folder下。\\n\\n存储的位置总是从working directory出发，而不是test.py的位置出发。\\n\\n使用Slash和back slash的方向在IBM和Mac中不一样，如果用r string则两个方向都可以，就像第二个存储路径的写法', metadata={'source': 'notes/文件存在哪里save file.docx'}),\n",
       "  0.331976056098938),\n",
       " (Document(page_content='Unnamed: 2 \\n0 \\\\\\n1  \\n2  \\n3  \\n4  \\n5  \\n6  \\n7  \\n8  \\n9  \\n10 tech stack \\n11 TFIDF, GloVe, bert, RNN, cosine similarity, BM25 \\n12Open source models GPT2, Bert, T5, Bart, GPT-J... \\n13 open source models + adapter \\n14 open source models + LangChain/GPT-index \\n15 OpenAI API \\n16 build plugin \\n17 \\n18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 这样的model可以回答domain之内和之外的问题 \\n27 \\n28 \\n29 \\n30 \\n31 老师finetune过generative task吗？ \\n32 \\n33 \\n34 \\n35 \\n36 \\n37 \\n38 \\n39 \\n40 \\n41 \\n42 \\n43 \\n44 \\n45 \\n46 \\n47 \\n48 \\n49 \\n50 \\n51 \\n52 \\n53 \\n54 \\n55 \\n56 \\n57 \\n58 \\n59 \\n60 \\n61 \\n62 \\n63 \\n64 \\n65 \\n66 \\n67 \\n68 \\n69 \\n70 \\n71 \\n72 \\n73 \\n74 \\n75 \\n76 \\n77 \\n78 \\n79 \\n80 \\n81 \\n82 \\n83 \\n84 \\n85 没找到能够读odp的方法，放弃odp \\n86 \\n87 \\n88 \\n89 \\n90 \\n91 \\n92 \\n93 \\n94 \\n95 \\n96 \\n97', metadata={'source': 'notes/目录.ods', 'sheet': 'Bert ELMo GPT'}),\n",
       "  0.5035257935523987),\n",
       " (Document(page_content='141 \\n142 \\n143 \\n144 \\n145 \\n146 \\n147 \\n148 \\n149 \\n150 \\n151 \\n152 \\n153 \\n154 \\n155 \\n156 \\n157 \\n158 \\n159 \\n160 \\n161 \\n162 \\n163 \\n164 \\n165 \\n166 MA: moving average \\n167 \\n168 \\n169 \\n170 \\n171 \\n172 \\n173 \\n174 \\n175 \\n176 \\n177 \\n178 \\n179 \\n180 \\n181 \\n182 \\n183 \\n184 \\n185 \\n186 \\n187 \\n188 \\n189 \\n190 \\n191 \\n192 \\n193 \\n194 \\n195 \\n196 \\n197 \\n198I: integrated, 就是差分项 I(d) \\n199 \\n200 \\n201', metadata={'source': 'notes/important notes.xlsx', 'sheet': 'ARIMA'}),\n",
       "  0.526513934135437),\n",
       " (Document(page_content='Unnamed: 8 Unnamed: 9 Unnamed: 10\\n0 \\n1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10  \\n11  \\n12  \\n13  \\n14  \\n15  \\n16  \\n17  \\n18  \\n19  \\n20  \\n21  \\n22  \\n23  \\n24  \\n25  \\n26  \\n27  \\n28  \\n29  \\n30  \\n31  \\n32  \\n33  \\n34  \\n35  \\n36  \\n37  \\n38  \\n39  \\n40  \\n41  \\n42  \\n43  \\n44  \\n45  \\n46  \\n47  \\n48  \\n49  \\n50  \\n51  \\n52  \\n53  \\n54  \\n55  \\n56  \\n57  \\n58  \\n59  \\n60  \\n61  \\n62  \\n63  \\n64  \\n65  \\n66  \\n67  \\n68  \\n69  \\n70  \\n71  \\n72  \\n73  \\n74  \\n75  \\n76  \\n77  \\n78  \\n79  \\n80  \\n81  \\n82  \\n83  \\n84  \\n85  \\n86  \\n87  \\n88  \\n89  \\n90  下载了slides\\n91  \\n92  \\n93  \\n94  \\n95  \\n96  \\n97  \\n98  \\n99  \\n100 \\n101 \\n102 \\n103 \\n104 \\n105 \\n106 \\n107 \\n108 \\n109 \\n110 \\n111 \\n112 \\n113 \\n114 \\n115 \\n116 \\n117 \\n118 \\n119 \\n120 \\n121 \\n122 \\n123 \\n124 \\n125 \\n126 \\n127 \\n128 \\n129 \\n130 \\n131 \\n132 \\n133 \\n134 \\n135 \\n136 \\n137 \\n138 \\n139 \\n140 \\n141 \\n142 \\n143 \\n144 \\n145 \\n146 \\n147 \\n148 \\n149 \\n150 \\n151 \\n152 生成句子对，有50%是上下文 \\n153给文本生成mask \\n154 \\n155 \\n156 \\n157 \\n158 \\n159 \\n160 \\n161 \\n162 \\n163 \\n164 \\n165 \\n166 \\n167 \\n168 \\n169 \\n170 \\n171 \\n172 \\n173 \\n174 \\n175 \\n176 \\n177 \\n178 \\n179 \\n180 \\n181 \\n182 \\n183 \\n184 \\n185 \\n186 \\n187 \\n188 \\n189', metadata={'source': 'notes/目录.ods', 'sheet': 'chap 14'}),\n",
       "  0.5395902395248413)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search_with_score(\"文件存在哪里\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To cleanup, you can delete the collection\n",
    "# vectordb.delete_collection()\n",
    "# vectordb.persist()\n",
    "\n",
    "# # Or just nuke the persist directory 如果需要删除db2：\n",
    "# !rm -rf db2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57724d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给chatbot加上memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9f99df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'db'\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58309387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install unstructured\n",
    "#brew install unoconv\n",
    "#pip install odfpy\n",
    "#pip install langchain\n",
    "#pip install chromadb\n",
    "#pip install pypdf\n",
    "\n",
    "import os\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "import subprocess  \n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQA, ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory, ConversationBufferMemory\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name=\"text-davinci-003\"\n",
    ")\n",
    "\n",
    "\n",
    "# chatbot QA function\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff', \n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "source_tool = Tool(\n",
    "    name=\"retrievalQA tool\",\n",
    "    func=qa.run,\n",
    "    description=\"Always check retrivealQA tool first when answering questions\"\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\")\n",
    "\n",
    "tools = [source_tool]\n",
    "\n",
    "conversational_agent = initialize_agent(\n",
    "    agent='conversational-react-description', \n",
    "    tools=tools, \n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "instruction = \" AI must use retrievalQA tool first. If cannot find answer in retrievalQA tool, AI's output must start with 'The question is not in the scope of the documents provided.', and then AI can answer the question based on chat history or parametric knowledge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f78cb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: retrievalQA tool\n",
      "Action Input: who is the author of Ancient and Modern Physics?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: The question is not in the scope of the documents provided. However, based on our previous conversation, I believe the author of Ancient and Modern Physics is Albert Einstein.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"who is the author of Ancient and Modern Physics? AI must use retrievalQA tool first. If cannot find answer in retrievalQA tool, AI's output must start with 'The question is not in the scope of the documents provided.', and then AI can answer the question based on chat history or parametric knowledge.\",\n",
       " 'chat_history': '',\n",
       " 'output': 'The question is not in the scope of the documents provided. However, based on our previous conversation, I believe the author of Ancient and Modern Physics is Albert Einstein.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who is the author of Ancient and Modern Physics?\" + instruction\n",
    "conversational_agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c018f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: retrievalQA tool\n",
      "Action Input: how to pre-train a transformer?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Pre-train a transformer using a simple denoising objective and then separately fine-tune on each of the downstream tasks. Use a standard encoder-decoder Transformer as proposed by Vaswani et al. (2017). Use teacher forcing and a cross-entropy loss for optimization. Pre-train for 524,288 steps on C4 with a maximum sequence length of 512 and a batch size of 128 sequences.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: This is a great way to pre-train a transformer. It is important to remember to use teacher forcing and a cross-entropy loss for optimization, and to pre-train for a large number of steps. Additionally, it is important to use a standard encoder-decoder Transformer as proposed by Vaswani et al. (2017).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"how to pre-train a transformer? AI must use retrievalQA tool first. If cannot find answer in retrievalQA tool, AI's output must start with 'The question is not in the scope of the documents provided.', and then AI can answer the question based on chat history or parametric knowledge.\",\n",
       " 'chat_history': '\\nThe human asked what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asked who is the author of Ancient and Modern Physics, and the AI used a retrievalQA tool before responding that the question was not in the scope of the documents provided, but based on the previous conversation, the author is Albert Einstein.',\n",
       " 'output': 'This is a great way to pre-train a transformer. It is important to remember to use teacher forcing and a cross-entropy loss for optimization, and to pre-train for a large number of steps. Additionally, it is important to use a standard encoder-decoder Transformer as proposed by Vaswani et al. (2017).'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"how to pre-train a transformer?\" + instruction\n",
    "conversational_agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2991f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Exploring the Limits of Transfer Learning\\n3.1 Baseline\\nOur goal for our baseline is to reﬂect typical, modern practice. We pre-train a standard\\nTransformer (described in Section 2.1) using a simple denoising objective and then separately\\nﬁne-tune on each of our downstream tasks. We describe the details of this experimental\\nsetup in the following subsections.\\n3.1.1 Model\\nFor our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al.\\n(2017). While many modern approaches to transfer learning for NLP use a Transformer\\narchitecture consisting of only a single “stack” (e.g. for language modeling (Radford et al.,\\n2018; Dong et al., 2019) or classiﬁcation and span prediction (Devlin et al., 2018; Yang et al.,\\n2019)), we found that using a standard encoder-decoder structure achieved good results\\non both generative and classiﬁcation tasks. We explore the performance of diﬀerent model\\narchitectures in Section 3.2.', metadata={'source': 'notes/T5.pdf', 'page': 10}),\n",
       "  0.40429896116256714),\n",
       " (Document(page_content='of parameters of BERT BASEsince our baseline model contains two layer stacks instead of\\none. For regularization, we use a dropout probability of 0.1everywhere dropout is applied\\nin the model.\\n3.1.2 Training\\nAs described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to\\nalways train using standard maximum likelihood, i.e. using teacher forcing (Williams and\\nZipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and\\nStern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability\\nlogit at every timestep).\\nWe pre-train each model for 219= 524,288steps on C4 before ﬁne-tuning. We use a\\nmaximum sequence length of 512and a batch size of 128sequences. Whenever possible,\\nwe “pack” multiple sequences into each entry of the batch10so that our batches contain\\nroughly 216= 65,536tokens. In total, this batch size and number of steps corresponds', metadata={'source': 'notes/T5.pdf', 'page': 10}),\n",
       "  0.47125551104545593),\n",
       " (Document(page_content='roughly 216= 65,536tokens. In total, this batch size and number of steps corresponds\\nto pre-training on 235≈34Btokens. This is considerably less than BERT (Devlin et al.,\\n2018), which used roughly 137Btokens, or RoBERTa (Liu et al., 2019c), which used roughly\\n2.2Ttokens. Using only 235tokens results in a reasonable computational budget while still\\nproviding a suﬃcient amount of pre-training for acceptable performance. We consider the\\n10.https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/\\nindex.html#data_generators.generator_utils.pack_examples\\n11', metadata={'source': 'notes/T5.pdf', 'page': 10}),\n",
       "  0.4951440095901489),\n",
       " (Document(page_content='architectures in Section 3.2.\\nOur baseline model is designed so that the encoder and decoder are each similar in\\nsize and conﬁguration to a “BERT BASE” (Devlin et al., 2018) stack. Speciﬁcally, both the\\nencoder and decoder consist of 12blocks (each block comprising self-attention, optional\\nencoder-decoder attention, and a feed-forward network). The feed-forward networks in each\\nblock consist of a dense layer with an output dimensionality of dﬀ= 3072followed by a\\nReLU nonlinearity and another dense layer. The “key” and “value” matrices of all attention\\nmechanisms have an inner dimensionality of dkv= 64and all attention mechanisms have 12\\nheads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total,\\nthis results in a model with about 220million parameters. This is roughly twice the number\\nof parameters of BERT BASEsince our baseline model contains two layer stacks instead of', metadata={'source': 'notes/T5.pdf', 'page': 10}),\n",
       "  0.5310637354850769)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search_with_score(\"how to pre-train a transformer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcfc8b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: retrievalQA tool\n",
      "Action Input: who did you think is the author of the physics book?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m I don't know.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: The question is not in the scope of the documents provided, but based on the previous conversation, the author is Albert Einstein.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"who did you think is the author of the physics book? AI must use retrievalQA tool first. If cannot find answer in retrievalQA tool, AI's output must start with 'The question is not in the scope of the documents provided.', and then AI can answer the question based on chat history or parametric knowledge.\",\n",
       " 'chat_history': '\\nThe human asked what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asked who is the author of Ancient and Modern Physics, and the AI used a retrievalQA tool before responding that the question was not in the scope of the documents provided, but based on the previous conversation, the author is Albert Einstein. The human then asked how to pre-train a transformer, and the AI suggested using teacher forcing and a cross-entropy loss for optimization, as well as a standard encoder-decoder Transformer as proposed by Vaswani et al. (2017).',\n",
       " 'output': 'The question is not in the scope of the documents provided, but based on the previous conversation, the author is Albert Einstein.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who did you think is the author of the physics book?\" + instruction\n",
    "conversational_agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e7b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
